{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Python Scientific Data Analysis\n",
    "## Course's Final Project\n",
    "### Barak Daniel - 204594329"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Installations needed for the program to run:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install seaborn\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn\n",
    "#!pip install sklearn\n",
    "#!pip install scipy\n",
    "#!pip install pydotplus\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pydotplus\n",
    "import random"
   ]
  },
  {
   "source": [
    "# Intro\n",
    "\n",
    "### Overview\n",
    "This project is about targetted marketing, in the data set given we have 'customer' as each row of data, and the features can tell us about the customer's status in general (age, marriage, etc..) and hes shopping behavior.\n",
    "In this section of the project I will go over the data set and the goal to try and understand the whole process before starting the actual work on the data.\n",
    "\n",
    "### So what exactly is targetted marketing?\n",
    "Targeting in marketing is a strategy that breaks a large market into smaller segments to concentrate on a specific group of customers within that audience. It defines a segment of customers based on their unique characteristics and focuses solely on serving them.\n",
    "Instead of trying to reach an entire market, a brand uses target marketing to put their energy into connecting with a specific, defined group within that market.\n",
    "So for this reason I'll break down all the features in the given dataset ('customers3.csv'), and understand them.\n",
    "\n",
    "\n",
    "### Feature's data breadown\n",
    "The following features are included in the data set given in 'customes3.csv':\n",
    "- ID - Unique ID to each customer\n",
    "- Gender - The gender of the customer\n",
    "- Ever_Marries - Indicates if the customer was married\n",
    "- Age - The age of the customer\n",
    "- Graduated - Has the customer graduated high school\n",
    "- Profession - The profession of the customer\n",
    "- Work_Experience - The number of years of the customer's expirence in his profession\n",
    "- Spending_Score - The spending habits of the customer classified to 3 categories\n",
    "- Family_Size - The number of family members the customer has in his household\n",
    "- Shop_Day - The day of the week which the customer is shopping on the most\n",
    "- Shop_Other - Normalized measure of customer deviation from average store customer spending on non specified products\n",
    "- Shop_Dairy - Normalized measure of customer deviation from average store customer spending on dairy products\n",
    "- Shop_Household: Normalized measure of customer deviation from average store customer spending on household products\n",
    "- Shop_Meat - Normalized measure of customer deviation from average store customer spending on meat products\n",
    "- Group - The target group which the customer belongs to\n",
    "\n",
    "### Feature's type breakdown\n",
    "- ID - Numerical discrete (Integer)\n",
    "- Gender - Categorical (Male/Female)\n",
    "- Ever_Marries - Categorical nominal (Yes/No)\n",
    "- Age - Numerical continuous (Integer)\n",
    "- Graduated - Categorical (Yes/No)\n",
    "- Profession - Categorical nominal\n",
    "- Work_Experience - Numerical discrete (Integer)\n",
    "- Spending_Score - Categorical ordinal (Low/average/High)\n",
    "- Family_Size - Numerical discrete (Integer)\n",
    "- Shop_Day - Categorical ordinal (Sunday, Monday, ..., Saturday)\n",
    "- Shop_Other - Numerical continuous (Double)\n",
    "- Shop_Dair - Numerical continuous (Double)\n",
    "- Shop_Household - Numerical continuous (Double)\n",
    "- Shop_Meat -Numerical continuous (Double)\n",
    "- Group - Categorical nominal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customers3.csv')\n",
    "count = df.count()\n",
    "\n",
    "print(\"The number of rows is: {}\".format(len(df.index)))\n",
    "print(\"The number of columns is: {}\".format(len(df.columns)))\n",
    "print(\"The number of cells is: {}\".format(len(df.index) * len(df.columns)))\n",
    "print(\"The number of cells with concrete values is: {}\".format(count.sum()))\n",
    "print(\"The number of cells without concrete values is: {}\\n\".format(len(df.index) * len(df.columns) - count.sum()))\n",
    "\n",
    "print(\"\\nThe number of concrete values for each feature:\")\n",
    "df.count()\n"
   ]
  },
  {
   "source": [
    "### The size of the data set is:\n",
    "- 8120 rows of customer's data (+1 for the headers of each column)\n",
    "- 15 columns for the features\n",
    "- 8120*15 = 121,8000 cells, but we can see that not all of them has concrete values.\n",
    "\n",
    "### Missing values:\n",
    "After watching the dataset and trying to understand it, I have also encountered many cells with missing data values.\n",
    "After reading the dataset a transformation of this Nan values is needed, for each feature with missing data, I'll examine it and understand which of the methods is the best to deal with those values (Mode, Mean, Median, Removal, etc..).\n",
    "\n",
    "### Other types of missing values:\n",
    "A validation for the values that are not missing must be made, after going through the features, the options are numeric value which is out of the range as given with feature definition, a numeric value that cannot be negative, etc...\n",
    "\n",
    "After going through out the dataset, those are the features needed to be fixed:\n",
    "- Shop_Day - Must contain values of 1 to 7 but there are values out of this range therefore it will be filled by the same method as all the feature values\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Initial Data Analysis\n",
    "\n",
    "As we saw above, there are a lot of missing values and categorical values we want to transform before we can alanyze the data completely.\n",
    "In this section of the project I will deal with those values, for each feature a check for the accuracy of the model will be taken and by that I can make the decision what was the best method for the feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first feature to handle missing data will be 'Gender', since we have less than 60 missing values and its binary the best method to do it is to check their distributions over the data set and then fill them with this distributions.\n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = 0\n",
    "female = 0\n",
    "for index,row in df.iterrows():\n",
    "    if(row['Gender'] == 'Male'):\n",
    "        male += 1\n",
    "    elif(row['Gender'] == 'Female'):\n",
    "        female += 1\n",
    "\n",
    "print(\"Female precentage\", female/(male+female))\n",
    "print(\"Male precentage\", male/(male+female))"
   ]
  },
  {
   "source": [
    "So we can see now that females represents ~45.25% of the rows in the data set and Males are ~54.75% .\n",
    "Now we fill the missing values with this distribution:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = df['Gender'].isna()\n",
    "length = sum(nans)\n",
    "replacement = random.choices(['Male', 'Female'], weights=[.5475, .4525], k=length)\n",
    "df.loc[nans,'Gender'] = replacement\n",
    "\n",
    "df.Gender.count()"
   ]
  },
  {
   "source": [
    "The next feature I'll be dealing with is Ever_Married which is also a binary answer of yes or no.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "married = 0\n",
    "notmarried = 0\n",
    "for index,row in df.iterrows():\n",
    "    if(row['Ever_Married'] == 'Yes'):\n",
    "        married += 1\n",
    "    elif(row['Ever_Married'] == 'No'):\n",
    "        notmarried += 1\n",
    "\n",
    "\n",
    "married_list = [\"Yes\", \"No\"]\n",
    "df[\"Ever_Married_Transformed\"] = pd.Categorical(df.Ever_Married, ordered=True, categories=married_list).codes + 1\n",
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Ever_Married_Transformed\"]))\n",
    "df.drop(['Ever_Married_Transformed', 'Group_Transformed'], axis=1)\n",
    "\n",
    "print(\"Percentage of married = \", (married/(married+notmarried)))\n",
    "print(\"Percentage of married = \", (notmarried/(married+notmarried)))\n",
    "\n",
    "df = df.drop([\"Ever_Married_Transformed\"], axis=1)"
   ]
  },
  {
   "source": [
    "The correlation to the group is low and therefore I can use the same method as I did in 'Gender'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = df['Ever_Married'].isna()\n",
    "length = sum(nans)\n",
    "replacement = random.choices(['Yes', 'No'], weights=[.5859, .4141], k=length)\n",
    "df.loc[nans,'Ever_Married'] = replacement\n",
    "\n",
    "df.Ever_Married.count()"
   ]
  },
  {
   "source": [
    "The next feature to deal with is 'Age', first I'll check some of the feature's data like the range and mean.\n",
    "Afterwards I fill the df with the values in the better method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Age\"]))\n",
    "\n",
    "ageMin = df.Age.min()\n",
    "ageMax = df.Age.max()\n",
    "ageMean = df.Age.mean()\n",
    "ageMedian = df.Age.median()\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\".format(ageMean, ageMedian))\n",
    "print(\"Min age is: {}  --- Max age is: {}\\n\".format(ageMin, ageMax))\n",
    "\n",
    "df[\"Mean_Age\"] = df.Age.fillna(ageMean)\n",
    "df[\"Median_Age\"] = df.Age.fillna(ageMedian)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_Age\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_Age\"]))"
   ]
  },
  {
   "source": [
    "As we can see, both of the values are nearly the same, but we will still prefer the median for the better correlation even if it is only slightly higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age = df.Median_Age\n",
    "df = df.drop(['Group_Transformed', \"Mean_Age\", \"Median_Age\"], axis=1)\n",
    "\n",
    "df.Age.count()"
   ]
  },
  {
   "source": [
    "The next feature will be Graduated which missing a few values, so like the other binary features I have dealt with above, I'll do the same here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = 0\n",
    "ungrad = 0\n",
    "for index,row in df.iterrows():\n",
    "    if(row['Graduated'] == 'Yes'):\n",
    "        grad += 1\n",
    "    elif(row['Graduated'] == 'No'):\n",
    "        ungrad += 1\n",
    "\n",
    "print(\"Graduated precentage\", ungrad/(grad+ungrad))\n",
    "print(\"Haven't graduated precentage\", grad/(grad+ungrad))\n",
    "\n",
    "nans = df['Graduated'].isna()\n",
    "length = sum(nans)\n",
    "replacement = random.choices(['Yes', 'No'], weights=[.3781, .6219], k=length)\n",
    "df.loc[nans,'Graduated'] = replacement\n",
    "\n",
    "df.Graduated.count()"
   ]
  },
  {
   "source": [
    "The next feature is Profession, the difference from the features we dealt with already is that this feature is Categorical nominal which the set of his values is not finite, therefore to fill this column and not lose the data from the rest of the rows, I will fill the values with Mode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proMode = df.Profession.mode()[0]\n",
    "df[\"Profession\"] = df.Profession.fillna(proMode)"
   ]
  },
  {
   "source": [
    "Now the feature to be dealt with is Work_Expirence, because there is the Age col, maybe here we can fill the missing values with deductive imputation, so now I'll check their corr and decide how to fill this feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.corr(df.Work_Experience)"
   ]
  },
  {
   "source": [
    "Because the corr is low, using the Age feature won't be good enough for filling those values, now I'll check the mathematical operation that can be done."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Work_Experience\"]))\n",
    "\n",
    "workMin = df.Work_Experience.min()\n",
    "workMax = df.Work_Experience.max()\n",
    "workMean = df.Work_Experience.mean()\n",
    "workMedian = df.Work_Experience.median()\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\".format(workMean, workMedian))\n",
    "print(\"Min work exp is: {}  --- Max work exp is: {}\\n\".format(workMin, workMax))\n",
    "\n",
    "df[\"Mean_work\"] = df.Work_Experience.fillna(workMean)\n",
    "df[\"Median_work\"] = df.Work_Experience.fillna(workMedian)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_work\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_work\"]))\n",
    "\n",
    "df[\"Work_Experience\"] = df.Work_Experience.fillna(workMean)\n",
    "df = df.drop(['Group_Transformed', \"Mean_work\", \"Median_work\"], axis=1)\n",
    "\n",
    "print(df.Work_Experience.count())"
   ]
  },
  {
   "source": [
    "As we can see in the above, the correlation that is most fitted here is filling the missing values with Mean.\n",
    "\n",
    "Now the next feature is \"family size\", for that feature I will test Mode, Median and Mean.\n",
    "For the mean option I will round the value of the mean so there will be a valid family size value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Family_Size\"]))\n",
    "\n",
    "familyMean = round(df.Family_Size.mean())\n",
    "familyMedian = df.Family_Size.median()\n",
    "familyMode = df.Family_Size.mode()[0]\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\\nMode = {}\\n\".format(familyMean, familyMedian, familyMode))\n",
    "\n",
    "df[\"Mean_family\"] = df.Family_Size.fillna(familyMean)\n",
    "df[\"Median_family\"] = df.Family_Size.fillna(familyMedian)\n",
    "df[\"Mode_family\"] = df.Family_Size.fillna(familyMode)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_family\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_family\"]))\n",
    "print(\"Corr with mode: \", df[\"Group_Transformed\"].corr(df[\"Mode_family\"]))\n",
    "\n",
    "df[\"Family_Size\"] = df.Family_Size.fillna(familyMean)\n",
    "df = df.drop(['Group_Transformed', \"Mean_family\", \"Median_family\", \"Mode_family\"], axis=1)\n",
    "\n",
    "print(df.Family_Size.count())"
   ]
  },
  {
   "source": [
    "Both mean and median gave the same result and got better correlation than mode, that is why I choose their value to fill the \"Family_Size\" feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the next feature, \"Shop_Day\", before filling the missing values I need to deal with the wrong values the feaure is containing:\n",
    "Must contain values of 1 to 7 but there are values out of this range."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Shop_Day.unique())"
   ]
  },
  {
   "source": [
    "The values that needs to be dealt with first are 0 and 22"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_Shop_Day = df.Shop_Day\n",
    "for index, value in enumerate(temp_Shop_Day):\n",
    "    if(value == 0 or value == 22):\n",
    "        temp_Shop_Day[index] = np.nan\n",
    "\n",
    "df.Shop_Day = temp_Shop_Day\n",
    "print(df.Shop_Day.unique())"
   ]
  },
  {
   "source": [
    "Now I can fill the missing values by checking the best mathematical operation without wrong values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Shop_Day\"]))\n",
    "\n",
    "dayMean = round(df.Shop_Day.mean())\n",
    "dayMedian = df.Shop_Day.median()\n",
    "dayMode = df.Shop_Day.mode()[0]\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\\nMode = {}\\n\".format(dayMean, dayMedian, dayMode))\n",
    "\n",
    "df[\"Mean_day\"] = df.Shop_Day.fillna(dayMean)\n",
    "df[\"Median_day\"] = df.Shop_Day.fillna(dayMedian)\n",
    "df[\"Mode_day\"] = df.Shop_Day.fillna(dayMode)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_day\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_day\"]))\n",
    "print(\"Corr with mode: \", df[\"Group_Transformed\"].corr(df[\"Mode_day\"]))\n",
    "\n",
    "df[\"Shop_Day\"] = df.Shop_Day.fillna(dayMean)\n",
    "df = df.drop(['Group_Transformed', \"Mean_day\", \"Median_day\", \"Mode_day\"], axis=1)\n",
    "\n",
    "print(df.Shop_Day.count())"
   ]
  },
  {
   "source": [
    "As we can see above the Mean filling option is better than median and mode which are sharing the same value, Therefore I chose to use it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Moving on to the next feature, \"Shop_Diary\", I will check all the fitting mathematical operations as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Shop_Dairy\"]))\n",
    "\n",
    "dairyMean = df.Shop_Dairy.mean()\n",
    "dairyMedian = df.Shop_Dairy.median()\n",
    "dairyMode = df.Shop_Dairy.mode()[0]\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\\nMode = {}\\n\".format(dairyMean, dairyMedian, dairyMode))\n",
    "\n",
    "df[\"Mean_dairy\"] = df.Shop_Dairy.fillna(dairyMean)\n",
    "df[\"Median_dairy\"] = df.Shop_Dairy.fillna(dairyMedian)\n",
    "df[\"Mode_dairy\"] = df.Shop_Dairy.fillna(dairyMode)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_dairy\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_dairy\"]))\n",
    "print(\"Corr with mode: \", df[\"Group_Transformed\"].corr(df[\"Mode_dairy\"]))\n",
    "\n",
    "df[\"Shop_Dairy\"] = df.Shop_Dairy.fillna(dairyMean)\n",
    "df = df.drop(['Group_Transformed', \"Mean_dairy\", \"Median_dairy\", \"Mode_dairy\"], axis=1)\n",
    "\n",
    "print(df.Shop_Dairy.count())"
   ]
  },
  {
   "source": [
    "The mean and median are pretty close in their correlation to the group, still mean is higher so That is why I chose to use it here as well. \n",
    "\n",
    "For the next 2 feature's, \"Shop_Household\" and \"Shop_Meat\", I will use the same method as in Dairy "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Shop_Household\"]))\n",
    "\n",
    "HouseholdMean = df.Shop_Household.mean()\n",
    "HouseholdMedian = df.Shop_Household.median()\n",
    "HouseholdMode = df.Shop_Household.mode()[0]\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\\nMode = {}\\n\".format(HouseholdMean, HouseholdMedian, HouseholdMode))\n",
    "\n",
    "df[\"Mean_Household\"] = df.Shop_Household.fillna(HouseholdMean)\n",
    "df[\"Median_Household\"] = df.Shop_Household.fillna(HouseholdMedian)\n",
    "df[\"Mode_Household\"] = df.Shop_Household.fillna(HouseholdMode)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_Household\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_Household\"]))\n",
    "print(\"Corr with mode: \", df[\"Group_Transformed\"].corr(df[\"Mode_Household\"]))\n",
    "\n",
    "df[\"Shop_Household\"] = df.Shop_Household.fillna(HouseholdMean)\n",
    "df = df.drop(['Group_Transformed', \"Mean_Household\", \"Median_Household\", \"Mode_Household\"], axis=1)\n",
    "\n",
    "print(df.Shop_Household.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Shop_Meat\"]))\n",
    "\n",
    "MeatMean = round(df.Shop_Meat.mean())\n",
    "MeatMedian = df.Shop_Meat.median()\n",
    "MeatMode = df.Shop_Meat.mode()[0]\n",
    "\n",
    "print(\"\\nAge aggregations:\\nMean = {}\\nMedian = {}\\nMode = {}\\n\".format(MeatMean, MeatMedian, MeatMode))\n",
    "\n",
    "df[\"Mean_Household\"] = df.Shop_Meat.fillna(MeatMean)\n",
    "df[\"Median_Household\"] = df.Shop_Meat.fillna(MeatMedian)\n",
    "df[\"Mode_Household\"] = df.Shop_Meat.fillna(MeatMode)\n",
    "\n",
    "\n",
    "print(\"Corr with mean: \", df[\"Group_Transformed\"].corr(df[\"Mean_Household\"]))\n",
    "print(\"Corr with median: \", df[\"Group_Transformed\"].corr(df[\"Median_Household\"]))\n",
    "print(\"Corr with mode: \", df[\"Group_Transformed\"].corr(df[\"Mode_Household\"]))\n",
    "\n",
    "df[\"Shop_Meat\"] = df.Shop_Meat.fillna(MeatMean)\n",
    "df = df.drop(['Group_Transformed', \"Mean_Household\", \"Median_Household\", \"Mode_Household\"], axis=1)\n",
    "\n",
    "print(df.Shop_Meat.count())"
   ]
  },
  {
   "source": [
    "Both \"Shop_Household\" and \"Shop_Meat\" will get the best result by filling with the mean value of the feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now that all the dataset is fixed I'll save it to a new csv."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "df.to_csv(\"customer3_fixed.csv\")"
   ]
  },
  {
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "feature correlation:\n",
    "- Turning categorical features to numeric representation\n",
    "- Ploting correlation map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"Male\", \"Female\"]\n",
    "df[\"Gender\"] = pd.Categorical(df.Gender, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Yes\", \"Now\"]\n",
    "df[\"Ever_Married\"] = pd.Categorical(df.Ever_Married, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Yes\", \"Now\"]\n",
    "df[\"Graduated\"] = pd.Categorical(df.Graduated, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = []\n",
    "uniqueProf = df.Profession.unique()\n",
    "for prof in uniqueProf:\n",
    "    group_list.append(prof)\n",
    "\n",
    "df[\"Profession\"] = pd.Categorical(df.Profession, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Low\", \"Average\", \"High\"]\n",
    "df[\"Spending_Score\"] = pd.Categorical(df.Spending_Score, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(21, 17))\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap = cmap, center=0, annot = True , square = True , linewidths = .6, vmin = -1, vmax = 1 )\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "What can we learn from the correlation heatmap?\n",
    "\n",
    "- ID: is just the id of the observation therefore there shouldn't be any relation for it to the other features.\n",
    "\n",
    "- Gender: this feature is not in a good correlation with Group, it can't help us learn much.\n",
    "\n",
    "- Ever_Married: comparing to the other values, there is a small negative correlation to Group which indicates that people who got ever married mostly got into an higher group.\n",
    "\n",
    "- Age: this is the 3rd best correlation to Group we can see in the dataset, this correlation can tell us that the younger the customer, the more likley he will be in a higher group.\n",
    "\n",
    "- Graduated: this feature's correlation to Group is average comparing to the other features and this small correlation\n",
    " indicates that people who did graduate will be more likeley to be in an higher group.\n",
    " \n",
    "- Profession: this feature is also on an average correlation to Group comparing to the other features, but we can't learn much from its correlation to Group since it is a nominal feature and can't really indicate a behavior.\n",
    "\n",
    "- Work_Expirence: The lowest correlation to Group comparing to all the other features, we can't learn from it anything relating to the target group.\n",
    "\n",
    "- Spending_Score: this feature is not so correlated to the Group target feature, but is still has some negative correlation, which indicates that the lower the shoping score of the customer the more likeley he will be in an higher group.\n",
    "\n",
    "- Family_Size: this feature has an average correlation to Group and we can learn from it that as the family size grow bigger, there is more chance to be in an higher group.\n",
    "\n",
    "- Shop_Day: the favorite shop day of the week as a very small correlation to Group and therefore we can't learn much from it.\n",
    "\n",
    "- Shop_Other: the 2nd best feature for correlation with Group, we learn from it that the more the customer is spending on other things than dairy, meat and household, the higher group he will probably be in.\n",
    "\n",
    "- Shop_Dairy: an average correlation compared to the other features but we can still learn that the more the customer spend on dairy the more likley he will be in an higher group.\n",
    "\n",
    "- Shop_Household: this feature has the best correlation to group in all the dataset, we can learn that the more the customer spends on household he will probably get into an higher group.\n",
    "\n",
    "- Shop_Meat: this feature has an average correlation comped to the other features and indicates a small connection between the amount a customer spend on meat and the group as the more he spend the higher group he will probably get into."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now I'll display the most relevant features in graphs:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df['Group']\n",
    "features = ['Shop_Household', 'Shop_Other', 'Age']\n",
    "\n",
    "for feat in features:\n",
    "    temp  = df[feat]\n",
    "    cor = group.corr(temp)\n",
    "    plt.title('Group {} correlation {}'.format(feat,cor))\n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel('Group')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10, 8)\n",
    "    plt.scatter(temp, group)\n",
    "    plt.plot(np.unique(temp), np.poly1d(np.polyfit(temp, group, 1))(np.unique(temp)), color='red', linewidth = 2.9 )\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "From the 3 graphs above we can see clearly that there is a linear correlation between each of these features to the Group feature.\n",
    "now I will show those features in a pair plot to see if there is more insights about their data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairPlotDf = df[['Shop_Household', 'Shop_Other', 'Age', 'Group']]\n",
    "sns.set_context(rc={\"axes.labelsize\":20})\n",
    "pairPlot = sns.pairplot(pairPlotDf, hue='Group', palette='Set1', corner=True)\n",
    "pairPlot.fig.set_size_inches(25, 25)\n",
    "pairPlot._legend.remove()\n",
    "plt.legend(title='Group', loc=(2., 1.5), labels=['A', 'B', 'C', 'D'], prop={'size': 20}, title_fontsize='25')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We can see that as the heatmap showed us that the most correlated features to the Group target feature are giving also the most clrear classification.\n",
    "In the pairplot above the Shop_Other and Shop_Household giving the best result for the scatter of the groups.\n",
    "\n",
    "The last feature I want to check in this part is the Profession because it is hard to know anything from it by looking at the correlation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairPlotDf = df[['Shop_Household', 'Shop_Other', 'Profession', 'Group']]\n",
    "sns.set_context(rc={\"axes.labelsize\":20})\n",
    "pairPlot = sns.pairplot(pairPlotDf, hue='Group', palette='Set1', corner=True)\n",
    "pairPlot.fig.set_size_inches(25, 25)\n",
    "pairPlot._legend.remove()\n",
    "plt.legend(title='Group', loc=(2., 1.5), labels=['A', 'B', 'C', 'D'], prop={'size': 20}, title_fontsize='25')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Now we can realy see that both the heatmap and the pairplot indicates that the Profession feature is not correlated to the Group feature, and since it is not related in a strong way to any other feature, there is no information we can get from it in this stage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}